{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "548d1a24",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# Распознавание текста\n",
    "\n",
    "## CRNN+CTC loss baseline\n",
    "\n",
    "В данном ноутбуке представлен baseline модели распознавания текста с помощью CRNN модели и CTC loss. Вы можете добавить новые аугментации или изменить структуру данной модели, или же попробовать совершенно новую архитектуру."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68e1dd95-aabd-4d8d-9577-313c3fe25087",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!unzip images.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a804d024",
   "metadata": {},
   "source": [
    "# 0. Установка и подгрузука библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ee595a",
   "metadata": {},
   "source": [
    "Установка библиотек, под которым запускается данный бейзлайн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcce067f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: albumentations in ./.imgenv-test-0/lib/python3.7/site-packages (1.1.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in ./.imgenv-test-0/lib/python3.7/site-packages (from albumentations) (4.5.5.62)\n",
      "Requirement already satisfied: PyYAML in /home/user/conda/lib/python3.7/site-packages (from albumentations) (6.0)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in ./.imgenv-test-0/lib/python3.7/site-packages (from albumentations) (0.19.2)\n",
      "Requirement already satisfied: qudida>=0.0.4 in ./.imgenv-test-0/lib/python3.7/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /home/user/conda/lib/python3.7/site-packages (from albumentations) (1.21.5)\n",
      "Requirement already satisfied: scipy in /home/user/conda/lib/python3.7/site-packages (from albumentations) (1.4.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /home/user/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions in /home/user/conda/lib/python3.7/site-packages (from qudida>=0.0.4->albumentations) (4.0.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in ./.imgenv-test-0/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.16.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (9.0.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in ./.imgenv-test-0/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in ./.imgenv-test-0/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2021.11.2)\n",
      "Requirement already satisfied: networkx>=2.2 in ./.imgenv-test-0/lib/python3.7/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/user/conda/lib/python3.7/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/user/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.0.0)\n",
      "Requirement already satisfied: asrtoolkit in ./.imgenv-test-0/lib/python3.7/site-packages (0.2.4)\n",
      "Requirement already satisfied: termcolor in /home/user/conda/lib/python3.7/site-packages (from asrtoolkit) (1.1.0)\n",
      "Requirement already satisfied: regex in /home/user/conda/lib/python3.7/site-packages (from asrtoolkit) (2022.1.18)\n",
      "Requirement already satisfied: editdistance in ./.imgenv-test-0/lib/python3.7/site-packages (from asrtoolkit) (0.6.0)\n",
      "Requirement already satisfied: fire in ./.imgenv-test-0/lib/python3.7/site-packages (from asrtoolkit) (0.4.0)\n",
      "Requirement already satisfied: beautifulsoup4 in /home/user/conda/lib/python3.7/site-packages (from asrtoolkit) (4.10.0)\n",
      "Requirement already satisfied: xlrd<=1.2.0 in ./.imgenv-test-0/lib/python3.7/site-packages (from asrtoolkit) (1.2.0)\n",
      "Requirement already satisfied: pytest in ./.imgenv-test-0/lib/python3.7/site-packages (from asrtoolkit) (7.0.1)\n",
      "Requirement already satisfied: tqdm in /home/user/conda/lib/python3.7/site-packages (from asrtoolkit) (4.62.3)\n",
      "Requirement already satisfied: webvtt-py in ./.imgenv-test-0/lib/python3.7/site-packages (from asrtoolkit) (0.4.6)\n",
      "Requirement already satisfied: num2words in ./.imgenv-test-0/lib/python3.7/site-packages (from asrtoolkit) (0.5.10)\n",
      "Requirement already satisfied: soupsieve>1.2 in /home/user/conda/lib/python3.7/site-packages (from beautifulsoup4->asrtoolkit) (2.3.1)\n",
      "Requirement already satisfied: six in /home/user/conda/lib/python3.7/site-packages (from fire->asrtoolkit) (1.16.0)\n",
      "Requirement already satisfied: docopt>=0.6.2 in ./.imgenv-test-0/lib/python3.7/site-packages (from num2words->asrtoolkit) (0.6.2)\n",
      "Requirement already satisfied: packaging in /home/user/conda/lib/python3.7/site-packages (from pytest->asrtoolkit) (21.3)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /home/user/conda/lib/python3.7/site-packages (from pytest->asrtoolkit) (4.10.1)\n",
      "Requirement already satisfied: pluggy<2.0,>=0.12 in ./.imgenv-test-0/lib/python3.7/site-packages (from pytest->asrtoolkit) (1.0.0)\n",
      "Requirement already satisfied: py>=1.8.2 in ./.imgenv-test-0/lib/python3.7/site-packages (from pytest->asrtoolkit) (1.11.0)\n",
      "Requirement already satisfied: iniconfig in ./.imgenv-test-0/lib/python3.7/site-packages (from pytest->asrtoolkit) (1.1.1)\n",
      "Requirement already satisfied: tomli>=1.0.0 in ./.imgenv-test-0/lib/python3.7/site-packages (from pytest->asrtoolkit) (2.0.1)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /home/user/conda/lib/python3.7/site-packages (from pytest->asrtoolkit) (21.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->asrtoolkit) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->asrtoolkit) (3.7.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/user/conda/lib/python3.7/site-packages (from packaging->pytest->asrtoolkit) (3.0.7)\n"
     ]
    }
   ],
   "source": [
    "#!pip install numpy\n",
    "#!pip install torchvision==0.11.3+cu102 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install opencv-python==4.5.2.52\n",
    "#!pip install matplotlib==3.4.2\n",
    "!pip install -U albumentations\n",
    "#!pip install opencv-python\n",
    "!pip install asrtoolkit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4dc81cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from asrtoolkit import cer\n",
    "from random import uniform\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e66674",
   "metadata": {},
   "source": [
    "## 1. Разделим трейн датасет на обучающую и валидационную подвыборки\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2b5a075",
   "metadata": {},
   "source": [
    "Сначала преобразуем таблицу (в которой есть колонка base_image) в `labels.json` - это формат из второго этапа олимпиады, для которого составлялся бейзлайн. По сути это просто словарь из колонок 'file_name' и 'text'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a5fcf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3d714213",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv = pd.read_csv('train_recognition/labels.csv')\n",
    "train_csv = train_csv.sample(frac = 1)\n",
    "\n",
    "train_data = dict(train_csv[['file_name','text']].values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "37b27c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train len 161700\n",
      "train len after split 160083\n",
      "val len after split 1617\n"
     ]
    }
   ],
   "source": [
    "train_data = [(k, v) for k, v in train_data.items()]\n",
    "print('train len', len(train_data))\n",
    "\n",
    "split_coef = 0.99\n",
    "train_len = int(len(train_data)*split_coef)\n",
    "\n",
    "train_data_splitted = train_data[:train_len]\n",
    "val_data_splitted = train_data[train_len:]\n",
    "\n",
    "print('train len after split', len(train_data_splitted))\n",
    "print('val len after split', len(val_data_splitted))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6307704a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8eacdebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_recognition/train_labels_splitted.json', 'w') as f:\n",
    "    json.dump(dict(train_data_splitted), f)\n",
    "    \n",
    "with open('train_recognition/val_labels_splitted.json', 'w') as f:\n",
    "    json.dump(dict(val_data_splitted), f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270c0c76",
   "metadata": {},
   "source": [
    "## 2. Зададим параметры обучения\n",
    "\n",
    "Здесь мы можем поправить конфиги обучения - задать размер батча, количество эпох, размер входных изображений, а также установить пути к датасетам."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7109653",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphabet = ''.join(sorted(list(set(''.join(train_csv['text'].values)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3539fc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "config_json = {\n",
    "    \"alphabet\": ''' !\"%\\'()*+,-./0123456789:;<=>?ABCDEFGHIJKLMNOPRSTUVWXY[]_abcdefghijklmnopqrstuvwxyz|}ЁАБВГДЕЖЗИКЛМНОПРСТУФХЦЧШЩЫЬЭЮЯабвгдежзийклмнопрстуфхцчшщъыьэюяё№''',\n",
    "    \"save_dir\": \"output\",\n",
    "    \"num_epochs\": 100,\n",
    "    \"image\": {\n",
    "        \"width\": 180,\n",
    "        \"height\": 100,\n",
    "    },\n",
    "    \"train\": {\n",
    "        \"root_path\": \"images/\",\n",
    "        \"json_path\": \"train_recognition/train_labels_splitted.json\",\n",
    "        \"batch_size\": 246\n",
    "    },\n",
    "    \"val\": {\n",
    "        \"root_path\": \"images/\",\n",
    "        \"json_path\": \"train_recognition/val_labels_splitted.json\",\n",
    "        \"batch_size\": 246\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8e2ddbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DEVICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "215e2da1-5ace-4e71-abb9-740b53c6da84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.6.0+cu101\n"
     ]
    }
   ],
   "source": [
    "# !nvidia-smi\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e04f96",
   "metadata": {},
   "source": [
    "## 3. Теперь определим класс датасета (torch.utils.data.Dataset) и другие вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b976c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция которая помогает объединять картинки и таргет-текст в батч\n",
    "def collate_fn(batch):\n",
    "    images, texts, enc_texts = zip(*batch)\n",
    "    images = torch.stack(images, 0)\n",
    "    text_lens = torch.LongTensor([len(text) for text in texts])\n",
    "    enc_pad_texts = pad_sequence(enc_texts, batch_first=True, padding_value=0)\n",
    "    return images, texts, enc_pad_texts, text_lens\n",
    "\n",
    "\n",
    "def get_data_loader(\n",
    "    transforms, json_path, root_path, tokenizer, batch_size, drop_last\n",
    "):\n",
    "    dataset = OCRDataset(json_path, root_path, tokenizer, transforms)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        collate_fn=collate_fn,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    return data_loader\n",
    "\n",
    "\n",
    "class OCRDataset(Dataset):\n",
    "    def __init__(self, json_path, root_path, tokenizer, transform=None):\n",
    "        super().__init__()\n",
    "        self.transform = transform\n",
    "        with open(json_path, 'r') as f:\n",
    "            data = json.load(f)\n",
    "        self.data_len = len(data)\n",
    "\n",
    "        self.img_paths = []\n",
    "        self.texts = []\n",
    "        for img_name, text in data.items():\n",
    "            self.img_paths.append(os.path.join(root_path, img_name))\n",
    "            self.texts.append(text)\n",
    "        self.enc_texts = tokenizer.encode(self.texts)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.data_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.img_paths[idx]\n",
    "        text = self.texts[idx]\n",
    "        enc_text = torch.LongTensor(self.enc_texts[idx])\n",
    "        image = cv2.imread(img_path)\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, text, enc_text\n",
    "\n",
    "\n",
    "class AverageMeter:\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b28fdb5",
   "metadata": {},
   "source": [
    "## 4. Здесь определен Токенайзер - вспопогательный класс, который преобразует текст в числа\n",
    "\n",
    "Разметка-текст с картинок преобразуется в числовое представление, на которых модель может учиться. Также может преобразовывать числовое предсказание модели обратно в текст."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d1648058",
   "metadata": {},
   "outputs": [],
   "source": [
    "OOV_TOKEN = '<OOV>'\n",
    "CTC_BLANK = '<BLANK>'\n",
    "\n",
    "\n",
    "def get_char_map(alphabet):\n",
    "    \"\"\"Make from string alphabet character2int dict.\n",
    "    Add BLANK char fro CTC loss and OOV char for out of vocabulary symbols.\"\"\"\n",
    "    char_map = {value: idx + 2 for (idx, value) in enumerate(alphabet)}\n",
    "    char_map[CTC_BLANK] = 0\n",
    "    char_map[OOV_TOKEN] = 1\n",
    "    return char_map\n",
    "\n",
    "\n",
    "class Tokenizer:\n",
    "    \"\"\"Class for encoding and decoding string word to sequence of int\n",
    "    (and vice versa) using alphabet.\"\"\"\n",
    "\n",
    "    def __init__(self, alphabet):\n",
    "        self.char_map = get_char_map(alphabet)\n",
    "        self.rev_char_map = {val: key for key, val in self.char_map.items()}\n",
    "\n",
    "    def encode(self, word_list):\n",
    "        \"\"\"Returns a list of encoded words (int).\"\"\"\n",
    "        enc_words = []\n",
    "        for word in word_list:\n",
    "            enc_words.append(\n",
    "                [self.char_map[char] if char in self.char_map\n",
    "                 else self.char_map[OOV_TOKEN]\n",
    "                 for char in word]\n",
    "            )\n",
    "        return enc_words\n",
    "\n",
    "    def get_num_chars(self):\n",
    "        return len(self.char_map)\n",
    "\n",
    "    def decode(self, enc_word_list):\n",
    "        \"\"\"Returns a list of words (str) after removing blanks and collapsing\n",
    "        repeating characters. Also skip out of vocabulary token.\"\"\"\n",
    "        dec_words = []\n",
    "        for word in enc_word_list:\n",
    "            word_chars = ''\n",
    "            for idx, char_enc in enumerate(word):\n",
    "                # skip if blank symbol, oov token or repeated characters\n",
    "                if (\n",
    "                    char_enc != self.char_map[OOV_TOKEN]\n",
    "                    and char_enc != self.char_map[CTC_BLANK]\n",
    "                    # idx > 0 to avoid selecting [-1] item\n",
    "                    and not (idx > 0 and char_enc == word[idx - 1])\n",
    "                ):\n",
    "                    word_chars += self.rev_char_map[char_enc]\n",
    "            dec_words.append(word_chars)\n",
    "        return dec_words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c6df21",
   "metadata": {},
   "source": [
    "## 5. Accuracy в качестве метрики\n",
    "\n",
    "Accuracy измеряет долю предсказанных строк текста, которые полностью совпадают с таргет текстом.\n",
    "### Не актуально\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "588929f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_accuracy(y_true, y_pred):\n",
    "#     scores = []\n",
    "#     for true, pred in zip(y_true, y_pred):\n",
    "#         scores.append(true == pred)\n",
    "#     avg_score = np.mean(scores)\n",
    "#     return avg_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0354972c-1744-4ff7-a107-a213a2e5edcc",
   "metadata": {},
   "source": [
    "### CER в качестве метрики\n",
    "\n",
    "используется стандартный CER из библиотеки torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "98f97cf1-466f-4681-b953-b7ed020e3825",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cer(y_true, y_pred):\n",
    "    scores = []\n",
    "    for true, pred in zip(y_true, y_pred):\n",
    "        scores.append(cer(true, pred)/100)\n",
    "    avg_score = np.mean(scores)\n",
    "    return avg_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "87ccb00b-de7d-49c5-a624-007d57575d72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5454545454545454"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cer(\"hello_world\", \"hello\")/100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb94641",
   "metadata": {},
   "source": [
    "## 6. Аугментации\n",
    "\n",
    "Здесь мы задаем базовые аугментации для модели. Вы можете написать свои или использовать готовые библиотеки типа albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "615419c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image, ImageDraw\n",
    "import albumentations as A\n",
    "\n",
    "class Normalize:\n",
    "    def __call__(self, img):\n",
    "        img = img.astype(np.float32) / 255\n",
    "        return img\n",
    "\n",
    "\n",
    "class ToTensor:\n",
    "    def __call__(self, arr):\n",
    "        arr = torch.from_numpy(arr)\n",
    "        return arr\n",
    "\n",
    "\n",
    "class MoveChannels:\n",
    "    \"\"\"Move the channel axis to the zero position as required in pytorch.\"\"\"\n",
    "\n",
    "    def __init__(self, to_channels_first=True):\n",
    "        self.to_channels_first = to_channels_first\n",
    "\n",
    "    def __call__(self, image):\n",
    "        if self.to_channels_first:\n",
    "            return np.moveaxis(image, -1, 0)\n",
    "        else:\n",
    "            return np.moveaxis(image, 0, -1)\n",
    "\n",
    "class ImageResize:\n",
    "    def __init__(self, height, width):\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = cv2.resize(image, (self.width, self.height),\n",
    "                           interpolation=cv2.INTER_LINEAR)\n",
    "        return image\n",
    "\n",
    "class ToNumpy:\n",
    "    def __call__(self, img):\n",
    "        return np.array(img)\n",
    "\n",
    "class ToPillow:\n",
    "    def __call__(self, array):\n",
    "        return Image.fromarray(array)\n",
    "\n",
    "class New_Aug:\n",
    "    def __init__(self):\n",
    "        self.aug = A.Compose([\n",
    "            A.RandomBrightnessContrast(p=0.2),\n",
    "            A.ISONoise(),\n",
    "        ])\n",
    "    \n",
    "    def __call__(self, img):\n",
    "        return self.aug(image=img)[\"image\"]\n",
    "\n",
    "class RandomLine:\n",
    "    def __init__(self, line_width=10, count=3):\n",
    "        self.line_width=line_width\n",
    "        self.count=count\n",
    "        \n",
    "    def __call__(self, img):\n",
    "        draw = ImageDraw.Draw(img)\n",
    "        width, height = img.size\n",
    "        for i in range(self.count):\n",
    "            draw.line((uniform(0, width/2), uniform(height/4, height*3/4), uniform(width/2, width), uniform(height/4, height*3/4)), fill='black', width=self.line_width)\n",
    "        return img\n",
    "    \n",
    "def get_train_transforms(height, width):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        ToPillow(),\n",
    "        RandomLine(),\n",
    "        torchvision.transforms.ColorJitter(brightness=.5, hue=.3,contrast= .4, saturation= 0.3),\n",
    "        ToNumpy(),\n",
    "        New_Aug(),\n",
    "        ImageResize(height, width),\n",
    "        MoveChannels(to_channels_first=True),\n",
    "        Normalize(),\n",
    "        ToTensor(),\n",
    "    ])\n",
    "    return transforms\n",
    "        \n",
    "def get_val_transforms(height, width):\n",
    "    transforms = torchvision.transforms.Compose([\n",
    "        ImageResize(height, width),\n",
    "        MoveChannels(to_channels_first=True),\n",
    "        Normalize(),\n",
    "        ToTensor()\n",
    "    ])\n",
    "    return transforms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dda2a878",
   "metadata": {},
   "source": [
    "## 7. Здесь определяем саму модель - CRNN\n",
    "\n",
    "Подробнее об архитектуре можно почитать в статье https://arxiv.org/abs/1507.05717"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5d85e1d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_backbone(pretrained=True):\n",
    "    m = torchvision.models.resnet50(pretrained=pretrained)\n",
    "    blocks = [m.conv1, m.bn1, m.relu, m.maxpool, m.layer1]\n",
    "    return nn.Sequential(*blocks)\n",
    "\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size, hidden_size, num_layers,\n",
    "            dropout=dropout, batch_first=True, bidirectional=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(\n",
    "        self, number_class_symbols, time_feature_count=256, lstm_hidden=256,\n",
    "        lstm_len=2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.feature_extractor = get_backbone(pretrained=True)\n",
    "        self.avg_pool = nn.AdaptiveAvgPool2d(\n",
    "            (time_feature_count, time_feature_count))\n",
    "        self.bilstm = BiLSTM(time_feature_count, lstm_hidden, lstm_len)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden * 2, time_feature_count),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(0.4),\n",
    "            nn.Linear(time_feature_count, number_class_symbols)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.feature_extractor(x)\n",
    "        b, c, h, w = x.size()\n",
    "        x = x.view(b, c * h, w)\n",
    "        x = self.avg_pool(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.bilstm(x)\n",
    "        x = self.classifier(x)\n",
    "        x = nn.functional.log_softmax(x, dim=2).permute(1, 0, 2)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b579f5a",
   "metadata": {},
   "source": [
    "## 8. Переходим к самому скрипту обучения - циклы трейна и валидации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d38b9c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_loop(data_loader, model, tokenizer, device):\n",
    "    acc_avg = AverageMeter()\n",
    "    for images, texts, _, _ in data_loader:\n",
    "        batch_size = len(texts)\n",
    "        text_preds = predict(images, model, tokenizer, device)\n",
    "        acc_avg.update(get_cer(texts, text_preds), batch_size)\n",
    "    print(f'Validation, CER: {acc_avg.avg:.4f}')\n",
    "    return acc_avg.avg\n",
    "\n",
    "\n",
    "def train_loop(data_loader, model, criterion, optimizer, epoch):\n",
    "    loss_avg = AverageMeter()\n",
    "    model.train()\n",
    "    for images, texts, enc_pad_texts, text_lens in data_loader:\n",
    "        model.zero_grad()\n",
    "        images = images.to(DEVICE)\n",
    "        batch_size = len(texts)\n",
    "        output = model(images)\n",
    "        output_lenghts = torch.full(\n",
    "            size=(output.size(1),),\n",
    "            fill_value=output.size(0),\n",
    "            dtype=torch.long\n",
    "        )\n",
    "        loss = criterion(output, enc_pad_texts, output_lenghts, text_lens)\n",
    "        loss_avg.update(loss.item(), batch_size)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 2)\n",
    "        optimizer.step()\n",
    "    for param_group in optimizer.param_groups:\n",
    "        lr = param_group['lr']\n",
    "    print(f'\\nEpoch {epoch}, Loss: {loss_avg.avg:.5f}, LR: {lr:.7f}')\n",
    "    return loss_avg.avg\n",
    "\n",
    "\n",
    "def predict(images, model, tokenizer, device):\n",
    "    model.eval()\n",
    "    images = images.to(device)\n",
    "    with torch.no_grad():\n",
    "        output = model(images)\n",
    "    pred = torch.argmax(output.detach().cpu(), -1).permute(1, 0).numpy()\n",
    "    text_preds = tokenizer.decode(pred)\n",
    "    return text_preds\n",
    "\n",
    "\n",
    "def get_loaders(tokenizer, config):\n",
    "    train_transforms = get_train_transforms(\n",
    "        height=config['image']['height'],\n",
    "        width=config['image']['width']\n",
    "    )\n",
    "    train_loader = get_data_loader(\n",
    "        json_path=config['train']['json_path'],\n",
    "        root_path=config['train']['root_path'],\n",
    "        transforms=train_transforms,\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=config['train']['batch_size'],\n",
    "        drop_last=True\n",
    "    )\n",
    "    val_transforms = get_val_transforms(\n",
    "        height=config['image']['height'],\n",
    "        width=config['image']['width']\n",
    "    )\n",
    "    val_loader = get_data_loader(\n",
    "        transforms=val_transforms,\n",
    "        json_path=config['val']['json_path'],\n",
    "        root_path=config['val']['root_path'],\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=config['val']['batch_size'],\n",
    "        drop_last=False\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def train(config, path_to_chekpoint=None):\n",
    "    tokenizer = Tokenizer(config['alphabet'])\n",
    "    os.makedirs(config['save_dir'], exist_ok=True)\n",
    "    train_loader, val_loader = get_loaders(tokenizer, config)\n",
    "    print(\"create model\")\n",
    "    model = CRNN(number_class_symbols=tokenizer.get_num_chars())\n",
    "    if path_to_chekpoint:\n",
    "        model.load_state_dict(torch.load(path_to_chekpoint, map_location=torch.device('cpu')))\n",
    "    model.to(DEVICE)\n",
    "    print(\"Succes\")\n",
    "\n",
    "    criterion = torch.nn.CTCLoss(blank=0, reduction='mean', zero_infinity=True)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=0.001,\n",
    "                                  weight_decay=0.01)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer=optimizer, mode='max', factor=0.5, patience=15)\n",
    "    best_acc = -np.inf\n",
    "    acc_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n",
    "    print(\"start train\")\n",
    "    for epoch in range(config['num_epochs']):\n",
    "        loss_avg = train_loop(train_loader, model, criterion, optimizer, epoch)\n",
    "        acc_avg = val_loop(val_loader, model, tokenizer, DEVICE)\n",
    "        scheduler.step(acc_avg)\n",
    "        best_acc = acc_avg\n",
    "        model_save_path = os.path.join(config['save_dir'], f'model-{epoch}-{acc_avg:.4f}.ckpt')\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        print('Model weights saved')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702a7f74",
   "metadata": {},
   "source": [
    "## 9. Запускаем обучение!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2623aea6",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create model\n",
      "Succes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-5:\n",
      "Process Process-6:\n",
      "Process Process-7:\n",
      "Process Process-8:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x7f17ed7fb290>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1101, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/user/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\", line 1075, in _shutdown_workers\n",
      "    w.join(timeout=_utils.MP_STATUS_CHECK_INTERVAL)\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/process.py\", line 140, in join\n",
      "    res = self._popen.wait(timeout)\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/popen_fork.py\", line 45, in wait\n",
      "    if not wait([self.sentinel], timeout):\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/connection.py\", line 921, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/home/user/conda/lib/python3.7/selectors.py\", line 415, in select\n",
      "    fd_event_list = self._selector.poll(timeout)\n",
      "KeyboardInterrupt: \n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/process.py\", line 300, in _bootstrap\n",
      "    util._exit_function()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 360, in _exit_function\n",
      "    _run_finalizers()\n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 300, in _run_finalizers\n",
      "    finalizer()\n",
      "KeyboardInterrupt\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/util.py\", line 224, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "KeyboardInterrupt\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/user/conda/lib/python3.7/multiprocessing/queues.py\", line 192, in _finalize_join\n",
      "    thread.join()\n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1044, in join\n",
      "    self._wait_for_tstate_lock()\n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "  File \"/home/user/conda/lib/python3.7/threading.py\", line 1060, in _wait_for_tstate_lock\n",
      "    elif lock.acquire(block, timeout):\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_143/4285263636.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_json\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output/model-63-0.0000.ckpt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_143/1096475568.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, path_to_chekpoint)\u001b[0m\n\u001b[1;32m     88\u001b[0m         optimizer=optimizer, mode='max', factor=0.5, patience=15)\n\u001b[1;32m     89\u001b[0m     \u001b[0mbest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0macc_avg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_loop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDEVICE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"start train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'num_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143/1096475568.py\u001b[0m in \u001b[0;36mval_loop\u001b[0;34m(data_loader, model, tokenizer, device)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mtext_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0macc_avg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mget_cer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Validation, CER: {acc_avg.avg:.4f}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143/1096475568.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(images, model, tokenizer, device)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mtext_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143/1334313503.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg_pool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbilstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_143/1334313503.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    720\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    724\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/user/conda/lib/python3.7/site-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbatch_sizes\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m             result = _VF.lstm(input, hx, self._flat_weights, self.bias, self.num_layers,\n\u001b[0;32m--> 577\u001b[0;31m                               self.dropout, self.training, self.bidirectional, self.batch_first)\n\u001b[0m\u001b[1;32m    578\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m             result = _VF.lstm(input, batch_sizes, hx, self._flat_weights, self.bias,\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "train(config_json, \"output/model-63-0.0000.ckpt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f4656d",
   "metadata": {},
   "source": [
    "## 10. Создание предсказаний для public-датасета\n",
    "\n",
    "Сначала определим класс для создания предсказаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d187224",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InferenceTransform:\n",
    "    def __init__(self, height, width):\n",
    "        self.transforms = get_val_transforms(height, width)\n",
    "\n",
    "    def __call__(self, images):\n",
    "        transformed_images = []\n",
    "        for image in images:\n",
    "            image = self.transforms(image)\n",
    "            transformed_images.append(image)\n",
    "        transformed_tensor = torch.stack(transformed_images, 0)\n",
    "        return transformed_tensor\n",
    "\n",
    "\n",
    "class OcrPredictor:\n",
    "    def __init__(self, model_path, config, device='cuda'):\n",
    "        self.tokenizer = Tokenizer(config['alphabet'])\n",
    "        self.device = torch.device(device)\n",
    "        # load model\n",
    "        self.model = CRNN(number_class_symbols=self.tokenizer.get_num_chars())\n",
    "        self.model.load_state_dict(torch.load(model_path))\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.transforms = InferenceTransform(\n",
    "            height=config['image']['height'],\n",
    "            width=config['image']['width'],\n",
    "        )\n",
    "\n",
    "    def __call__(self, images):\n",
    "        if isinstance(images, (list, tuple)):\n",
    "            one_image = False\n",
    "        elif isinstance(images, np.ndarray):\n",
    "            images = [images]\n",
    "            one_image = True\n",
    "        else:\n",
    "            raise Exception(f\"Input must contain np.ndarray, \"\n",
    "                            f\"tuple or list, found {type(images)}.\")\n",
    "\n",
    "        images = self.transforms(images)\n",
    "        pred = predict(images, self.model, self.tokenizer, self.device)\n",
    "\n",
    "        if one_image:\n",
    "            return pred[0]\n",
    "        else:\n",
    "            return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a3cc7c",
   "metadata": {},
   "source": [
    "Инициализируем OCR predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec949ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = OcrPredictor(\n",
    "    model_path='experiments/test/model-99-0.7310.ckpt',\n",
    "    config=config_json\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b0255b",
   "metadata": {},
   "source": [
    "Посмотрим несколько предсказаний и создадим финальный json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab792ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pred_json = {}\n",
    "\n",
    "print_images = True\n",
    "for val_img in val_data_splitted[22:25]:\n",
    "    img = cv2.imread(f'train_recognition/images/{val_img[0]}')\n",
    "\n",
    "    pred = predictor(img)\n",
    "    pred_json[val_img[0]] = pred\n",
    "\n",
    "    if print_images:\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "        print('Prediction: ', predictor(img))\n",
    "        print('True: ', val_img[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790f2fe",
   "metadata": {},
   "source": [
    "Сохраням submission json с предсказаниями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3264ac64",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prediction_HTR.json', 'w') as f:\n",
    "    json.dump(pred_json, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d4c20d-4ac8-4142-8453-d9bc9a163282",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
