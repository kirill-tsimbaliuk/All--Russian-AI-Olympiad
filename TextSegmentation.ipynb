{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Сегментация тетрадей\n",
    "\n",
    "## Detectron2 baseline\n",
    "\n",
    "В данном ноутбуке представлен baseline модели сегментации текста в школьных тетрадях с помощью фреймворка detectron2. Вы можете (и это даже лучше) использовать другие модели (например UNET, mmdet), или написать полностью свою."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Установка библиотек"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установка библиотек, под которым запускается данный бейзлайн."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://download.pytorch.org/whl/torch_stable.html\n",
      "Requirement already satisfied: torch==1.6.0+cu101 in /home/user/conda/lib/python3.7/site-packages (1.6.0+cu101)\n",
      "Requirement already satisfied: torchvision==0.7.0+cu101 in /home/user/conda/lib/python3.7/site-packages (0.7.0+cu101)\n",
      "Requirement already satisfied: future in /home/user/conda/lib/python3.7/site-packages (from torch==1.6.0+cu101) (0.18.2)\n",
      "Requirement already satisfied: numpy in /home/user/conda/lib/python3.7/site-packages (from torch==1.6.0+cu101) (1.21.5)\n",
      "Requirement already satisfied: pillow>=4.1.1 in /home/user/conda/lib/python3.7/site-packages (from torchvision==0.7.0+cu101) (9.0.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in links: https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html\n",
      "Requirement already satisfied: detectron2 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (0.4+cu101)\n",
      "Requirement already satisfied: termcolor>=1.1 in /home/user/conda/lib/python3.7/site-packages (from detectron2) (1.1.0)\n",
      "Requirement already satisfied: pycocotools>=2.0.2 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from detectron2) (2.0.4)\n",
      "Requirement already satisfied: fvcore<0.1.4,>=0.1.3 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from detectron2) (0.1.3.post20210317)\n",
      "Requirement already satisfied: pydot in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from detectron2) (1.4.2)\n",
      "Requirement already satisfied: tensorboard in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from detectron2) (2.1.1)\n",
      "Requirement already satisfied: matplotlib in /home/user/conda/lib/python3.7/site-packages (from detectron2) (3.5.1)\n",
      "Requirement already satisfied: iopath>=0.1.2 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from detectron2) (0.1.9)\n",
      "Requirement already satisfied: future in /home/user/conda/lib/python3.7/site-packages (from detectron2) (0.18.2)\n",
      "Requirement already satisfied: omegaconf>=2 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from detectron2) (2.1.1)\n",
      "Requirement already satisfied: tqdm>4.29.0 in /home/user/conda/lib/python3.7/site-packages (from detectron2) (4.62.3)\n",
      "Requirement already satisfied: Pillow>=7.1 in /home/user/conda/lib/python3.7/site-packages (from detectron2) (9.0.0)\n",
      "Requirement already satisfied: yacs>=0.1.6 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from detectron2) (0.1.8)\n",
      "Requirement already satisfied: tabulate in /home/user/conda/lib/python3.7/site-packages (from detectron2) (0.8.9)\n",
      "Requirement already satisfied: cloudpickle in /home/user/conda/lib/python3.7/site-packages (from detectron2) (2.0.0)\n",
      "Requirement already satisfied: numpy in /home/user/conda/lib/python3.7/site-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (1.21.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/user/conda/lib/python3.7/site-packages (from fvcore<0.1.4,>=0.1.3->detectron2) (6.0)\n",
      "Requirement already satisfied: portalocker in /home/user/conda/lib/python3.7/site-packages (from iopath>=0.1.2->detectron2) (2.3.2)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.8 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from omegaconf>=2->detectron2) (4.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (0.11.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (21.3)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (4.28.5)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (3.0.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/user/conda/lib/python3.7/site-packages (from matplotlib->detectron2) (1.3.2)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (2.0.2)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (0.4.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.0.0)\n",
      "Requirement already satisfied: six>=1.10.0 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.16.0)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.35.0)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (3.19.3)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (3.3.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (2.27.1)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /home/user/conda/lib/python3.7/site-packages (from tensorboard->detectron2) (1.43.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorboard->detectron2) (0.37.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorboard->detectron2) (60.5.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.2.4)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/user/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/user/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard->detectron2) (4.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/user/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/user/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard->detectron2) (4.10.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2.0.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard->detectron2) (1.26.8)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->detectron2) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard->detectron2) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/user/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard->detectron2) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->detectron2) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!python -m pip install detectron2 -f https://dl.fbaipublicfiles.com/detectron2/wheels/cu101/torch1.6/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow==2.1.0 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (2.1.0)\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.0.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.16.0)\n",
      "Requirement already satisfied: keras-preprocessing>=1.1.0 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.1.2)\n",
      "Requirement already satisfied: gast==0.2.2 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.2.2)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.43.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.2.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.1.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorflow==2.1.0) (2.1.0)\n",
      "Requirement already satisfied: astor>=0.6.0 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.8.1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.21.5)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (3.3.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorflow==2.1.0) (0.37.1)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.13.3)\n",
      "Requirement already satisfied: scipy==1.4.1 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.4.1)\n",
      "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorflow==2.1.0) (2.1.1)\n",
      "Requirement already satisfied: protobuf>=3.8.0 in /home/user/conda/lib/python3.7/site-packages (from tensorflow==2.1.0) (3.19.3)\n",
      "Requirement already satisfied: keras-applications>=1.0.8 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorflow==2.1.0) (1.0.8)\n",
      "Requirement already satisfied: h5py in /home/user/conda/lib/python3.7/site-packages (from keras-applications>=1.0.8->tensorflow==2.1.0) (2.10.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /home/user/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.0.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/user/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3.6)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /home/user/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.6)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/user/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.27.1)\n",
      "Requirement already satisfied: google-auth<2,>=1.6.3 in /home/user/conda/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.35.0)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /home/jovyan/.imgenv-test-0/lib/python3.7/site-packages (from tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (60.5.0)\n",
      "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.2.4)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/user/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.8)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/user/conda/lib/python3.7/site-packages (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/user/conda/lib/python3.7/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.3.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /home/user/conda/lib/python3.7/site-packages (from markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.10.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests<3,>=2.21.0->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (2.0.10)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (4.0.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/user/conda/lib/python3.7/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/user/conda/lib/python3.7/site-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/user/conda/lib/python3.7/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow==2.1.0) (3.1.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow==2.1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /home/user/conda/lib/python3.7/site-packages (4.5.5.62)\n",
      "Requirement already satisfied: numpy>=1.14.5 in /home/user/conda/lib/python3.7/site-packages (from opencv-python) (1.21.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Загрузить необходимые библиотеки для создания и обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import random\n",
    "import json\n",
    "import os\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interact_manual\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "import detectron2\n",
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.utils.visualizer import Visualizer\n",
    "from detectron2.data import MetadataCatalog,DatasetCatalog\n",
    "from detectron2.data.datasets import register_coco_instances,load_coco_json\n",
    "from detectron2.data import detection_utils as utils\n",
    "from detectron2.engine import DefaultTrainer\n",
    "from detectron2.engine import HookBase\n",
    "\n",
    "# from detectron2.utils.logger import setup_logger\n",
    "# setup_logger()\n",
    "\n",
    "import logging\n",
    "logger = logging.getLogger('detectron2')\n",
    "logger.setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прежде чем переходить к загрузке данных посмотрим, доступны ли нам GPU-мощности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU: True\n"
     ]
    }
   ],
   "source": [
    "print('GPU: ' + str(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Валидационный датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для валидации наших моделей нам неплохо было создать из обучающих данных валидационный датасет. Для этого разделим наш датасет на две части - для обучения и для валидации. Для этого просто создадим два новых файлика с аннотациями, куда раздельно запишем исиходную информацию об аннотациях."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "#Подгрузим аннотации train\n",
    "with open('train_segmentation/annotations.json') as f:\n",
    "    annotations = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['categories', 'images', 'annotations'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "annotations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "932"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['images'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "161700"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(annotations['annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Пустой словарь для аннотаций валидации\n",
    "annotations_val = {}\n",
    "#Список категорий такой же как в train\n",
    "annotations_val['categories'] = annotations['categories']\n",
    "\n",
    "#Пустой словарь для аннотаций нового train\n",
    "annotations_train = {}\n",
    "#Список категорий такой же как в train\n",
    "annotations_train['categories'] = annotations['categories']\n",
    "\n",
    "\n",
    "#Положим в валидацию каждое 12 изображение из исходного train, а остальные - в новый train\n",
    "annotations_val['images'] = []\n",
    "annotations_train['images'] = []\n",
    "for num,img in enumerate(annotations['images']):\n",
    "    if num%12==0:\n",
    "        annotations_val['images'].append(img)\n",
    "    else:\n",
    "        annotations_train['images'].append(img)\n",
    "\n",
    "#Положим в список аннотаций валидации только те аннотации, которые относятся к изображениям из валидации. \n",
    "#А в список аннотаций нового train - только те, которые относятся к нему\n",
    "val_img_id = [i['id'] for i in annotations_val['images']]\n",
    "train_img_id = [i['id'] for i in annotations_train['images']]\n",
    "\n",
    "annotations_val['annotations'] = []\n",
    "annotations_train['annotations'] = []\n",
    "\n",
    "for annot in annotations['annotations']:\n",
    "    if annot['image_id'] in val_img_id:\n",
    "        annotations_val['annotations'].append(annot)\n",
    "    elif annot['image_id'] in train_img_id:\n",
    "        annotations_train['annotations'].append(annot)\n",
    "    else:\n",
    "        print('Аннотации нет ни в одном наборе')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Готово! Аннотации для валидации и новой обучающей выборки готовы, теперь просто сохраним их в формате json, и положим в папке. Назовем аннотации annotations_new.json, чтобы новая набор аннотаций для train (без множества val) не перезаписал исходные аннотации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запишем новые файлы с аннотациями для train и val."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train_segmentation/annotations_val.json', 'w') as outfile:\n",
    "    json.dump(annotations_val, outfile)\n",
    "    \n",
    "    \n",
    "with open('train_segmentation/annotations_train.json', 'w') as outfile:\n",
    "    json.dump(annotations_train, outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Регистрация датасета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Зарегистрируем выборки в detectron2 для дальнейшей подачи на обучение модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in ['train','val']:\n",
    "    DatasetCatalog.register(\"my_dataset_\"+d, lambda d=d: load_coco_json(\"train_segmentation/annotations_{}.json\".format(d),\n",
    "    image_root= \"train_segmentation/images\",\\\n",
    "    dataset_name=\"my_dataset_\"+d,extra_annotation_keys=['bbox_mode']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "После регистрации можно загружать выборки, чтобы иметь возможность посмотреть на них глазами. Первой загрузим обучающую выборку в **dataset_dicts_train**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts_train = DatasetCatalog.get(\"my_dataset_train\")\n",
    "train_metadata = MetadataCatalog.get(\"my_dataset_train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И тестовую выборку в **dataset_dicts_val**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dicts_val = DatasetCatalog.get(\"my_dataset_val\")\n",
    "val_metadata = MetadataCatalog.get(\"my_dataset_val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на размер получившихся выборок - эта операция в python осуществляется при помощи функции **len()**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер обучающей выборки (Картинки): 854\n",
      "Размер тестовой выборки (Картинки): 78\n"
     ]
    }
   ],
   "source": [
    "print('Размер обучающей выборки (Картинки): {}'.format(len(dataset_dicts_train)))\n",
    "print('Размер тестовой выборки (Картинки): {}'.format(len(dataset_dicts_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, у нас в распоряжении 588 изображения для тренировки, и 66 - для проверки качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Посмотрим на размеченные фотографии из валидации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5de8380a62149dc81e3c3043faf0c60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(Dropdown(description='file', options=(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from IPython.display import Image\n",
    "@interact\n",
    "def show_images(file=range(len(dataset_dicts_val))):\n",
    "    example = dataset_dicts_val[file]\n",
    "    image = utils.read_image(example[\"file_name\"], format=\"RGB\")\n",
    "    plt.figure(figsize=(3,3),dpi=200)\n",
    "    visualizer = Visualizer(image[:, :, ::-1], metadata=val_metadata, scale=0.5)\n",
    "    vis = visualizer.draw_dataset_dict(example)\n",
    "    plt.imshow(vis.get_image()[:, :,::-1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##   4 Обучение модели\n",
    "\n",
    "**4.1. Определяем конфигурацию**\n",
    "\n",
    "Прежде чем начать работать с самой моделью, нам нужно определить ее параметры и спецификацию обучения\n",
    "\n",
    "Создаем конфигурацию и загружаем архитектуру модели с предобученными весами (на COCO - датасете, содержащем $80$ популярных категорий объектов и более $300000$ изображений) для распознавания объектов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = get_cfg()\n",
    "cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")) \n",
    "cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом, вы можете посмотреть и другие архитектуры в зоопарке [моделей](https://github.com/facebookresearch/detectron2/blob/master/MODEL_ZOO.md)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь задаем параметры самой модели и обучения модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем названия обучающией и тестовой выборок в настройки\n",
    "cfg.DATASETS.TRAIN = (\"my_dataset_train\",)\n",
    "cfg.DATASETS.TEST = (\"my_dataset_val\",)\n",
    "\n",
    "# Часто имеет смысл сделать изображения чуть меньшего размера, чтобы \n",
    "# обучение происходило быстрее. Поэтому мы можем указать размер, до которого будем изменяться наименьшая \n",
    "# и наибольшая из сторон исходного изображения.\n",
    "cfg.INPUT.MIN_SIZE_TRAIN = 3000\n",
    "cfg.INPUT.MAX_SIZE_TRAIN = 3000\n",
    "\n",
    "# Также мы должны сказать модели ниже какой вероятности определения она игнорирует результат. \n",
    "# То есть, если она найдет на картинке еду, но вероятность правильного определения ниже 0.5, \n",
    "# то она не будет нам сообщать, что она что-то нашла.\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "\n",
    "# Также мы должны указать порядок каналов во входном изображении. Обратите внимание, что это Blue Green Red (BGR), \n",
    "# а не привычный RGB. Это особенности работы данной модели.\n",
    "cfg.INPUT.FORMAT = 'BGR' \n",
    "\n",
    "# Для более быстрой загрузки данных в модель, мы делаем параллельную загрузку. Мы указываем параметр 4, \n",
    "cfg.DATALOADER.NUM_WORKERS = 2\n",
    "\n",
    "# Следующий параметр задает количество изображений в батче, на котором \n",
    "# модель делает одну итерацию обучения (изменения весов).\n",
    "cfg.SOLVER.IMS_PER_BATCH = 1\n",
    "\n",
    "# Зададим также learning_rate\n",
    "cfg.SOLVER.BASE_LR = 0.00025\n",
    "\n",
    "# Укажем модели, через сколько шагов обучения модели следует уменьшить learning rate\n",
    "cfg.SOLVER.STEPS = (2100, 6800, 8600,)\n",
    "\n",
    "# Фактор, на который уменьшается learning rate задается следующим выражением\n",
    "cfg.SOLVER.GAMMA = 0.1\n",
    "\n",
    "# Зададим общее число итераций обучения.\n",
    "cfg.SOLVER.MAX_ITER = 30000\n",
    "\n",
    "# Укажем количество классов в нашей выборке\n",
    "cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "\n",
    "# Задаем через сколько  шагов обучения сохранять веса модели в файл. Этот файл мы сможем загрузить потом \n",
    "# для тестирования нашей обученной модели на новых данных.\n",
    "cfg.SOLVER.CHECKPOINT_PERIOD = 1000\n",
    "\n",
    "# И указываем название папки, куда сохранять чекпойнты модели и информацию о процессе обучения.\n",
    "cfg.OUTPUT_DIR = './output'\n",
    "\n",
    "# Если вдруг такой папки нет, то создадим ее\n",
    "os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Если мы хотим удалить чекпойнты предыдущих моделей, то выполняем данную команду. \n",
    "#%rm output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. Обучаем модель**\n",
    "\n",
    "Процесс обучения модели запускают следующие три строчки кода. Возможно будут предупреждения, на которые можно не обращать внимания, это информация об обучении."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[03/01 21:24:21 d2.engine.defaults]: \u001b[0mModel:\n",
      "GeneralizedRCNN(\n",
      "  (backbone): FPN(\n",
      "    (fpn_lateral2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral3): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral4): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (fpn_lateral5): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
      "    (fpn_output5): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (top_block): LastLevelMaxPool()\n",
      "    (bottom_up): ResNet(\n",
      "      (stem): BasicStem(\n",
      "        (conv1): Conv2d(\n",
      "          3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False\n",
      "          (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "        )\n",
      "      )\n",
      "      (res2): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=64, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res3): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            256, 128, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=128, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res4): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            512, 256, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (3): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (4): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (5): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=256, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=1024, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (res5): Sequential(\n",
      "        (0): BottleneckBlock(\n",
      "          (shortcut): Conv2d(\n",
      "            1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "          (conv1): Conv2d(\n",
      "            1024, 512, kernel_size=(1, 1), stride=(2, 2), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (1): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "        (2): BottleneckBlock(\n",
      "          (conv1): Conv2d(\n",
      "            2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv2): Conv2d(\n",
      "            512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=512, eps=1e-05)\n",
      "          )\n",
      "          (conv3): Conv2d(\n",
      "            512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
      "            (norm): FrozenBatchNorm2d(num_features=2048, eps=1e-05)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (proposal_generator): RPN(\n",
      "    (rpn_head): StandardRPNHead(\n",
      "      (conv): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "      (objectness_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
      "      (anchor_deltas): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "    (anchor_generator): DefaultAnchorGenerator(\n",
      "      (cell_anchors): BufferList()\n",
      "    )\n",
      "  )\n",
      "  (roi_heads): StandardROIHeads(\n",
      "    (box_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(7, 7), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(7, 7), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(7, 7), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(7, 7), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (box_head): FastRCNNConvFCHead(\n",
      "      (flatten): Flatten()\n",
      "      (fc1): Linear(in_features=12544, out_features=1024, bias=True)\n",
      "      (fc_relu1): ReLU()\n",
      "      (fc2): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (fc_relu2): ReLU()\n",
      "    )\n",
      "    (box_predictor): FastRCNNOutputLayers(\n",
      "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
      "      (bbox_pred): Linear(in_features=1024, out_features=4, bias=True)\n",
      "    )\n",
      "    (mask_pooler): ROIPooler(\n",
      "      (level_poolers): ModuleList(\n",
      "        (0): ROIAlign(output_size=(14, 14), spatial_scale=0.25, sampling_ratio=0, aligned=True)\n",
      "        (1): ROIAlign(output_size=(14, 14), spatial_scale=0.125, sampling_ratio=0, aligned=True)\n",
      "        (2): ROIAlign(output_size=(14, 14), spatial_scale=0.0625, sampling_ratio=0, aligned=True)\n",
      "        (3): ROIAlign(output_size=(14, 14), spatial_scale=0.03125, sampling_ratio=0, aligned=True)\n",
      "      )\n",
      "    )\n",
      "    (mask_head): MaskRCNNConvUpsampleHead(\n",
      "      (mask_fcn1): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn2): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn3): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (mask_fcn4): Conv2d(\n",
      "        256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)\n",
      "        (activation): ReLU()\n",
      "      )\n",
      "      (deconv): ConvTranspose2d(256, 256, kernel_size=(2, 2), stride=(2, 2))\n",
      "      (deconv_relu): ReLU()\n",
      "      (predictor): Conv2d(256, 1, kernel_size=(1, 1), stride=(1, 1))\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\u001b[32m[03/01 21:24:24 d2.data.datasets.coco]: \u001b[0mLoading train_segmentation/annotations_train.json takes 2.89 seconds.\n",
      "\u001b[32m[03/01 21:24:24 d2.data.datasets.coco]: \u001b[0mLoaded 854 images in COCO format from train_segmentation/annotations_train.json\n",
      "\u001b[32m[03/01 21:24:26 d2.data.build]: \u001b[0mRemoved 0 images with no usable annotations. 854 images left.\n",
      "\u001b[32m[03/01 21:24:26 d2.data.dataset_mapper]: \u001b[0m[DatasetMapper] Augmentations used in training: [ResizeShortestEdge(short_edge_length=(3000, 3000), max_size=3000, sample_style='choice'), RandomFlip()]\n",
      "\u001b[32m[03/01 21:24:26 d2.data.build]: \u001b[0mUsing training sampler TrainingSampler\n",
      "\u001b[32m[03/01 21:24:26 d2.data.common]: \u001b[0mSerializing 854 elements to byte tensors and concatenating them all ...\n",
      "\u001b[32m[03/01 21:24:26 d2.data.common]: \u001b[0mSerialized dataset takes 67.82 MiB\n",
      "\u001b[32m[03/01 21:24:27 d2.engine.train_loop]: \u001b[0mStarting training from iteration 8000\n",
      "\u001b[32m[03/01 21:24:39 d2.utils.events]: \u001b[0m eta: 3:31:42  iter: 8019  total_loss: 0.9429  loss_cls: 0.2255  loss_box_reg: 0.3224  loss_mask: 0.2628  loss_rpn_cls: 0.009821  loss_rpn_loc: 0.1234  time: 0.5763  data_time: 0.0434  lr: 2.5e-06  max_mem: 22670M\n",
      "\u001b[32m[03/01 21:24:51 d2.utils.events]: \u001b[0m eta: 3:37:19  iter: 8039  total_loss: 0.986  loss_cls: 0.2519  loss_box_reg: 0.3467  loss_mask: 0.2515  loss_rpn_cls: 0.01837  loss_rpn_loc: 0.127  time: 0.5937  data_time: 0.0199  lr: 2.5e-06  max_mem: 23881M\n",
      "\u001b[32m[03/01 21:25:03 d2.utils.events]: \u001b[0m eta: 3:37:59  iter: 8059  total_loss: 0.9506  loss_cls: 0.2167  loss_box_reg: 0.3234  loss_mask: 0.2535  loss_rpn_cls: 0.01992  loss_rpn_loc: 0.12  time: 0.5912  data_time: 0.0152  lr: 2.5e-06  max_mem: 24272M\n",
      "\u001b[32m[03/01 21:25:15 d2.utils.events]: \u001b[0m eta: 3:37:41  iter: 8079  total_loss: 1.052  loss_cls: 0.2912  loss_box_reg: 0.3487  loss_mask: 0.2628  loss_rpn_cls: 0.01842  loss_rpn_loc: 0.137  time: 0.5908  data_time: 0.0200  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:25:26 d2.utils.events]: \u001b[0m eta: 3:35:13  iter: 8099  total_loss: 0.9583  loss_cls: 0.2277  loss_box_reg: 0.3249  loss_mask: 0.254  loss_rpn_cls: 0.009785  loss_rpn_loc: 0.1215  time: 0.5847  data_time: 0.0129  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:25:38 d2.utils.events]: \u001b[0m eta: 3:35:23  iter: 8119  total_loss: 1.025  loss_cls: 0.2591  loss_box_reg: 0.344  loss_mask: 0.2665  loss_rpn_cls: 0.01115  loss_rpn_loc: 0.1197  time: 0.5844  data_time: 0.0130  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:25:49 d2.utils.events]: \u001b[0m eta: 3:35:18  iter: 8139  total_loss: 1.001  loss_cls: 0.2332  loss_box_reg: 0.321  loss_mask: 0.2663  loss_rpn_cls: 0.0122  loss_rpn_loc: 0.1336  time: 0.5851  data_time: 0.0155  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:26:01 d2.utils.events]: \u001b[0m eta: 3:35:22  iter: 8159  total_loss: 0.9401  loss_cls: 0.2217  loss_box_reg: 0.3223  loss_mask: 0.2621  loss_rpn_cls: 0.01338  loss_rpn_loc: 0.1172  time: 0.5859  data_time: 0.0162  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:26:13 d2.utils.events]: \u001b[0m eta: 3:35:27  iter: 8179  total_loss: 0.9672  loss_cls: 0.2317  loss_box_reg: 0.3219  loss_mask: 0.2736  loss_rpn_cls: 0.008395  loss_rpn_loc: 0.1237  time: 0.5852  data_time: 0.0156  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:26:24 d2.utils.events]: \u001b[0m eta: 3:34:58  iter: 8199  total_loss: 0.9367  loss_cls: 0.2242  loss_box_reg: 0.327  loss_mask: 0.2389  loss_rpn_cls: 0.01933  loss_rpn_loc: 0.1172  time: 0.5838  data_time: 0.0134  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:26:36 d2.utils.events]: \u001b[0m eta: 3:35:03  iter: 8219  total_loss: 1.017  loss_cls: 0.2366  loss_box_reg: 0.3352  loss_mask: 0.2584  loss_rpn_cls: 0.01947  loss_rpn_loc: 0.1191  time: 0.5826  data_time: 0.0101  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:26:47 d2.utils.events]: \u001b[0m eta: 3:34:51  iter: 8239  total_loss: 0.927  loss_cls: 0.216  loss_box_reg: 0.3237  loss_mask: 0.2541  loss_rpn_cls: 0.01223  loss_rpn_loc: 0.1183  time: 0.5810  data_time: 0.0126  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:26:59 d2.utils.events]: \u001b[0m eta: 3:33:54  iter: 8259  total_loss: 1.017  loss_cls: 0.2586  loss_box_reg: 0.3545  loss_mask: 0.2735  loss_rpn_cls: 0.005421  loss_rpn_loc: 0.1236  time: 0.5808  data_time: 0.0161  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:27:10 d2.utils.events]: \u001b[0m eta: 3:32:46  iter: 8279  total_loss: 0.9982  loss_cls: 0.2351  loss_box_reg: 0.3581  loss_mask: 0.2646  loss_rpn_cls: 0.008777  loss_rpn_loc: 0.1118  time: 0.5796  data_time: 0.0120  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:27:22 d2.utils.events]: \u001b[0m eta: 3:33:03  iter: 8299  total_loss: 0.9337  loss_cls: 0.2153  loss_box_reg: 0.3231  loss_mask: 0.2607  loss_rpn_cls: 0.01528  loss_rpn_loc: 0.1135  time: 0.5802  data_time: 0.0193  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:27:33 d2.utils.events]: \u001b[0m eta: 3:32:47  iter: 8319  total_loss: 1.013  loss_cls: 0.2476  loss_box_reg: 0.3191  loss_mask: 0.2553  loss_rpn_cls: 0.01769  loss_rpn_loc: 0.1294  time: 0.5807  data_time: 0.0174  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:27:45 d2.utils.events]: \u001b[0m eta: 3:32:35  iter: 8339  total_loss: 1.009  loss_cls: 0.2625  loss_box_reg: 0.3344  loss_mask: 0.2547  loss_rpn_cls: 0.01274  loss_rpn_loc: 0.1131  time: 0.5819  data_time: 0.0180  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:27:57 d2.utils.events]: \u001b[0m eta: 3:32:28  iter: 8359  total_loss: 0.9427  loss_cls: 0.2053  loss_box_reg: 0.3313  loss_mask: 0.2541  loss_rpn_cls: 0.01332  loss_rpn_loc: 0.1123  time: 0.5828  data_time: 0.0165  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:28:09 d2.utils.events]: \u001b[0m eta: 3:32:08  iter: 8379  total_loss: 0.9175  loss_cls: 0.2319  loss_box_reg: 0.309  loss_mask: 0.2326  loss_rpn_cls: 0.009569  loss_rpn_loc: 0.1183  time: 0.5812  data_time: 0.0141  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:28:20 d2.utils.events]: \u001b[0m eta: 3:32:04  iter: 8399  total_loss: 1.003  loss_cls: 0.2315  loss_box_reg: 0.335  loss_mask: 0.2702  loss_rpn_cls: 0.008018  loss_rpn_loc: 0.1321  time: 0.5812  data_time: 0.0187  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:28:32 d2.utils.events]: \u001b[0m eta: 3:31:39  iter: 8419  total_loss: 0.9708  loss_cls: 0.2395  loss_box_reg: 0.3295  loss_mask: 0.253  loss_rpn_cls: 0.01202  loss_rpn_loc: 0.122  time: 0.5805  data_time: 0.0086  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:28:43 d2.utils.events]: \u001b[0m eta: 3:31:33  iter: 8439  total_loss: 1.004  loss_cls: 0.2666  loss_box_reg: 0.3527  loss_mask: 0.2594  loss_rpn_cls: 0.01666  loss_rpn_loc: 0.1286  time: 0.5803  data_time: 0.0130  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:28:55 d2.utils.events]: \u001b[0m eta: 3:31:21  iter: 8459  total_loss: 0.924  loss_cls: 0.2192  loss_box_reg: 0.3272  loss_mask: 0.2588  loss_rpn_cls: 0.01108  loss_rpn_loc: 0.1198  time: 0.5801  data_time: 0.0166  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:29:06 d2.utils.events]: \u001b[0m eta: 3:31:13  iter: 8479  total_loss: 1.031  loss_cls: 0.2547  loss_box_reg: 0.3392  loss_mask: 0.263  loss_rpn_cls: 0.01692  loss_rpn_loc: 0.1195  time: 0.5807  data_time: 0.0171  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:29:18 d2.utils.events]: \u001b[0m eta: 3:31:12  iter: 8499  total_loss: 0.9125  loss_cls: 0.2147  loss_box_reg: 0.3061  loss_mask: 0.2657  loss_rpn_cls: 0.01589  loss_rpn_loc: 0.1193  time: 0.5806  data_time: 0.0141  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:29:30 d2.utils.events]: \u001b[0m eta: 3:31:12  iter: 8519  total_loss: 0.9451  loss_cls: 0.2334  loss_box_reg: 0.3229  loss_mask: 0.2475  loss_rpn_cls: 0.01174  loss_rpn_loc: 0.1202  time: 0.5809  data_time: 0.0142  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:29:42 d2.utils.events]: \u001b[0m eta: 3:30:54  iter: 8539  total_loss: 1.022  loss_cls: 0.2523  loss_box_reg: 0.3421  loss_mask: 0.266  loss_rpn_cls: 0.01554  loss_rpn_loc: 0.1279  time: 0.5813  data_time: 0.0143  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:29:54 d2.utils.events]: \u001b[0m eta: 3:30:49  iter: 8559  total_loss: 0.9507  loss_cls: 0.2288  loss_box_reg: 0.3138  loss_mask: 0.2542  loss_rpn_cls: 0.006837  loss_rpn_loc: 0.1174  time: 0.5820  data_time: 0.0159  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:30:05 d2.utils.events]: \u001b[0m eta: 3:30:37  iter: 8579  total_loss: 0.99  loss_cls: 0.2269  loss_box_reg: 0.345  loss_mask: 0.2776  loss_rpn_cls: 0.006516  loss_rpn_loc: 0.1226  time: 0.5818  data_time: 0.0162  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:30:17 d2.utils.events]: \u001b[0m eta: 3:30:38  iter: 8599  total_loss: 0.9821  loss_cls: 0.2376  loss_box_reg: 0.3378  loss_mask: 0.2628  loss_rpn_cls: 0.01473  loss_rpn_loc: 0.1232  time: 0.5818  data_time: 0.0148  lr: 2.5e-06  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:30:29 d2.utils.events]: \u001b[0m eta: 3:30:30  iter: 8619  total_loss: 1.04  loss_cls: 0.2371  loss_box_reg: 0.3539  loss_mask: 0.2696  loss_rpn_cls: 0.008405  loss_rpn_loc: 0.129  time: 0.5823  data_time: 0.0126  lr: 2.5e-07  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:30:40 d2.utils.events]: \u001b[0m eta: 3:30:19  iter: 8639  total_loss: 0.9842  loss_cls: 0.255  loss_box_reg: 0.3265  loss_mask: 0.2638  loss_rpn_cls: 0.01545  loss_rpn_loc: 0.1293  time: 0.5822  data_time: 0.0122  lr: 2.5e-07  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:30:52 d2.utils.events]: \u001b[0m eta: 3:30:07  iter: 8659  total_loss: 0.8992  loss_cls: 0.2186  loss_box_reg: 0.3023  loss_mask: 0.2635  loss_rpn_cls: 0.01306  loss_rpn_loc: 0.1142  time: 0.5820  data_time: 0.0161  lr: 2.5e-07  max_mem: 25640M\n",
      "\u001b[32m[03/01 21:30:53 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <detectron2.modeling.matcher.Matcher object at 0x7fd32aca2310> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:31:00 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <detectron2.modeling.matcher.Matcher object at 0x7fd32aca2310> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:31:04 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:31:25 d2.utils.events]: \u001b[0m eta: 3:29:42  iter: 8679  total_loss: 1.004  loss_cls: 0.2357  loss_box_reg: 0.3299  loss_mask: 0.2525  loss_rpn_cls: 0.012  loss_rpn_loc: 0.1228  time: 0.6140  data_time: 0.0161  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:31:31 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:31:49 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:32:04 d2.utils.events]: \u001b[0m eta: 3:29:49  iter: 8699  total_loss: 1.026  loss_cls: 0.2291  loss_box_reg: 0.3318  loss_mask: 0.2765  loss_rpn_cls: 0.01722  loss_rpn_loc: 0.1225  time: 0.6520  data_time: 0.0174  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:32:15 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:32:30 d2.utils.events]: \u001b[0m eta: 3:29:38  iter: 8719  total_loss: 0.9895  loss_cls: 0.255  loss_box_reg: 0.3442  loss_mask: 0.2552  loss_rpn_cls: 0.01074  loss_rpn_loc: 0.1178  time: 0.6699  data_time: 0.0176  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:32:33 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:32:47 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:33:02 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:33:18 d2.utils.events]: \u001b[0m eta: 3:29:34  iter: 8739  total_loss: 0.9582  loss_cls: 0.2268  loss_box_reg: 0.3228  loss_mask: 0.254  loss_rpn_cls: 0.02763  loss_rpn_loc: 0.1165  time: 0.7172  data_time: 0.0154  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:33:25 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:33:44 d2.utils.events]: \u001b[0m eta: 3:29:15  iter: 8759  total_loss: 0.9766  loss_cls: 0.2182  loss_box_reg: 0.3352  loss_mask: 0.2496  loss_rpn_cls: 0.01867  loss_rpn_loc: 0.1221  time: 0.7323  data_time: 0.0104  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:33:54 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:34:08 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:34:20 d2.utils.events]: \u001b[0m eta: 3:29:06  iter: 8779  total_loss: 0.9692  loss_cls: 0.2349  loss_box_reg: 0.3235  loss_mask: 0.247  loss_rpn_cls: 0.01678  loss_rpn_loc: 0.1242  time: 0.7597  data_time: 0.0163  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:34:30 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:34:43 d2.utils.events]: \u001b[0m eta: 3:29:06  iter: 8799  total_loss: 0.9466  loss_cls: 0.2146  loss_box_reg: 0.3226  loss_mask: 0.2559  loss_rpn_cls: 0.0114  loss_rpn_loc: 0.1173  time: 0.7699  data_time: 0.0215  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:34:44 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:34:59 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:35:21 d2.utils.events]: \u001b[0m eta: 3:28:54  iter: 8819  total_loss: 0.9606  loss_cls: 0.2319  loss_box_reg: 0.3426  loss_mask: 0.2532  loss_rpn_cls: 0.01429  loss_rpn_loc: 0.116  time: 0.7973  data_time: 0.0205  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:35:25 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:35:46 d2.utils.events]: \u001b[0m eta: 3:28:49  iter: 8839  total_loss: 0.9687  loss_cls: 0.2264  loss_box_reg: 0.3211  loss_mask: 0.2656  loss_rpn_cls: 0.01181  loss_rpn_loc: 0.1264  time: 0.8079  data_time: 0.0145  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:35:46 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:36:00 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:36:23 d2.utils.events]: \u001b[0m eta: 3:28:26  iter: 8859  total_loss: 0.9327  loss_cls: 0.2351  loss_box_reg: 0.3167  loss_mask: 0.2598  loss_rpn_cls: 0.02085  loss_rpn_loc: 0.1275  time: 0.8318  data_time: 0.0131  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:36:24 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:36:40 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:37:00 d2.utils.events]: \u001b[0m eta: 3:28:14  iter: 8879  total_loss: 1.017  loss_cls: 0.2629  loss_box_reg: 0.3469  loss_mask: 0.269  loss_rpn_cls: 0.01761  loss_rpn_loc: 0.1207  time: 0.8557  data_time: 0.0148  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:37:03 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:37:23 d2.utils.events]: \u001b[0m eta: 3:28:03  iter: 8899  total_loss: 0.9292  loss_cls: 0.2449  loss_box_reg: 0.3056  loss_mask: 0.2549  loss_rpn_cls: 0.0124  loss_rpn_loc: 0.1195  time: 0.8622  data_time: 0.0208  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:37:24 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:37:48 d2.utils.events]: \u001b[0m eta: 3:27:55  iter: 8919  total_loss: 0.9524  loss_cls: 0.2477  loss_box_reg: 0.3232  loss_mask: 0.2583  loss_rpn_cls: 0.01391  loss_rpn_loc: 0.1245  time: 0.8703  data_time: 0.0158  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:37:49 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:38:06 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:38:27 d2.utils.events]: \u001b[0m eta: 3:27:35  iter: 8939  total_loss: 0.9134  loss_cls: 0.2155  loss_box_reg: 0.3219  loss_mask: 0.2494  loss_rpn_cls: 0.01605  loss_rpn_loc: 0.1206  time: 0.8937  data_time: 0.0109  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:38:29 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:38:55 d2.utils.events]: \u001b[0m eta: 3:27:19  iter: 8959  total_loss: 0.9422  loss_cls: 0.2178  loss_box_reg: 0.3181  loss_mask: 0.2444  loss_rpn_cls: 0.01152  loss_rpn_loc: 0.1266  time: 0.9042  data_time: 0.0129  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:39:02 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n",
      "\u001b[32m[03/01 21:39:20 d2.utils.events]: \u001b[0m eta: 3:27:12  iter: 8979  total_loss: 0.955  loss_cls: 0.24  loss_box_reg: 0.3269  loss_mask: 0.2577  loss_rpn_cls: 0.01396  loss_rpn_loc: 0.1238  time: 0.9106  data_time: 0.0163  lr: 2.5e-07  max_mem: 26655M\n",
      "\u001b[32m[03/01 21:39:23 d2.utils.memory]: \u001b[0mAttempting to copy inputs of <function pairwise_iou at 0x7fd37c46cd40> to CPU due to CUDA OOM\n"
     ]
    }
   ],
   "source": [
    "trainer = DefaultTrainer(cfg)\n",
    "trainer.resume_or_load(resume=True)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используем обученную модель для проверки качества на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg.MODEL.WEIGHTS = \"output/model_0002999.pth\"\n",
    "\n",
    "cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.5\n",
    "cfg.DATASETS.TEST = (\"my_dataset_val\", )\n",
    "#Изменение размера исходных изображений для тестового датасета\n",
    "cfg.INPUT.MIN_SIZE_TEST= 300\n",
    "cfg.INPUT.MAX_SIZE_TEST = 300\n",
    "cfg.INPUT.FORMAT = 'BGR'\n",
    "\n",
    "#ВАЖНО увеличить это значение (стандартное равно 100). Так как на листе тетради может быть довольно много слов\n",
    "cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
    "\n",
    "predictor = DefaultPredictor(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сделаем предсказания для тестового датасета и сразу же нарисуем его.\n",
    "\n",
    "Вы можете выбрать из выпадающего списка номер изображения, и посмотреть разметку на всем валидационном датасете."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@interact\n",
    "def show_images(file=range(len(dataset_dicts_val))):\n",
    "    \n",
    "    example = dataset_dicts_val[file]\n",
    "    im = cv2.imread(example[\"file_name\"])\n",
    "    outputs = predictor(im)\n",
    "    fig, axs = plt.subplots(nrows=1,ncols=2,figsize=(4,4),dpi=200)\n",
    "    v = Visualizer(im[:, :],\n",
    "                  metadata=val_metadata, \n",
    "                  scale=0.4 )\n",
    "    v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "    axs[0].imshow(im[:, :, ::-1])\n",
    "    axs[1].imshow(v.get_image()[:, :, ::-1])\n",
    "    axs[0].axis('off')\n",
    "    axs[1].axis('off')\n",
    "    axs[0].set_title('Original')\n",
    "    axs[1].set_title('Predict')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Можно непосредственно в коде изменить номер изображения, которое Вы хотите обработать."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_image_selected = 2\n",
    "example = dataset_dicts_val[id_image_selected]\n",
    "im = cv2.imread(example[\"file_name\"])\n",
    "outputs = predictor(im)\n",
    "plt.figure(figsize=(7,7))\n",
    "v = Visualizer(im[:, :],\n",
    "              metadata=val_metadata, \n",
    "              scale=0.4 )\n",
    "v = v.draw_instance_predictions(outputs[\"instances\"].to(\"cpu\"))\n",
    "plt.imshow(v.get_image()[:, :, ::-1])\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В качестве предсказаний для каждого изображения из тестового набора требуется получить бинарную маску, в которой `1` означает, что данный пиксель относится к классу текста."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте на примере одного изображения переведем формат выхода Detectron2 в требуемый формат для соревнования.\n",
    "\n",
    "`outputs` - результат предсказания модели на данном изображении из предыдущего блока с кодом"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = outputs['instances'].pred_masks.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В `prediction` находится массив бинарных матриц. Каждая матрица отвечает за отдельную задетектированную маску текста. В нашем случае модель задетектировала 80 текстовых масок. Давайте провизуализируем одну из них."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(prediction[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отлично. Теперь, для того, чтобы получить бинарную маску со всем задетектированным текстом для изображения, нам нужно объединить все маски в одну. Для этого мы просто поэелементно сложим все наши матрицы. Там, где после сложения остались нули - модель не задетектировала никакого текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = np.add.reduce(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = mask > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нам нужно полуить такую маску для каждого изображения из валидационной выборки, а затем посчитать метрику F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Подгрузим аннотации train\n",
    "with open('train_segmentation/annotations_val.json') as f:\n",
    "    annotations_val = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_images = annotations_val['images']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for val_img in tqdm.tqdm_notebook(val_images):\n",
    "    file_name = val_img['file_name']\n",
    "    img_path = os.path.join('train_segmentation/images/',file_name)\n",
    "    im = cv2.imread(img_path)\n",
    "    outputs = predictor(im)\n",
    "    prediction = outputs['instances'].pred_masks.cpu().numpy()\n",
    "    mask = np.add.reduce(prediction)\n",
    "    mask = mask > 0\n",
    "    val_predictions[file_name] = mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для сохрания предсказаний и загрузки бинарных масок бы будет использовать формат `.npz`. Он позволяет хранить большие массивы в компактном виде. Вот [ссылка](https://numpy.org/doc/stable/reference/generated/numpy.savez_compressed.html) на документацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('val_pred.npz',**val_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подгрузим бинарные маски для train и val (только что сохраненную). Так как мы в начале бейзлайна разбивали весь исходный train на новый трейн и валидацию, то информация по всем маскам из исходного train хранится в `binary.npz`. \n",
    "\n",
    "Получившийся после подгрузки `np.load()` - что то вроде словаря. Его ключи можно получить с помощью метода files - `loaded_val.files`. В нашем случае ключами являются ключи исходного словаря `val_predictions`, то есть названия изображений."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_train = np.load('train_segmentation/binary.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_val_pred = np.load('val_pred.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Мы используем среднюю метрика F1-score. То есть считаем F1-score для каждого изображения, а затем усредняем результаты. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация из sklearn работает довольно долго, поэтому мы будем использовать свою."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1_loss(y_true, y_pred):\n",
    "        \n",
    "    \n",
    "    tp = np.sum(y_true & y_pred)\n",
    "    tn = np.sum(~y_true & ~y_pred)\n",
    "    fp = np.sum(~y_true & y_pred)\n",
    "    fn = np.sum(y_true & ~y_pred)\n",
    "    \n",
    "    epsilon = 1e-7\n",
    "    \n",
    "    precision = tp / (tp + fp + epsilon)\n",
    "    recall = tp / (tp + fn + epsilon)\n",
    "    \n",
    "    f1 = 2* precision*recall / ( precision + recall + epsilon)\n",
    "\n",
    "    return f1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f1_scores = []\n",
    "for key in tqdm.tqdm_notebook(loaded_val_pred.files):\n",
    "    pred = loaded_val_pred[key].reshape(-1)\n",
    "    true = loaded_train[key].reshape(-1)\n",
    "    \n",
    "    f1_img = f1_loss(true,pred)\n",
    "    f1_scores.append(f1_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получившаяся метрика на валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(f1_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Запись submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как уже говорилось ранее, правильные и предсказанные маски мы будем хранить в компактном формате `npz`. \n",
    "\n",
    "Возьмем нашу обученную модель и запишем предсказания в файл `prediction.npz`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from detectron2 import model_zoo\n",
    "from detectron2.engine import DefaultPredictor\n",
    "from detectron2.config import get_cfg\n",
    "from detectron2.structures import Boxes, BoxMode \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(test_images_path,predictions_output_path):\n",
    "    threshold = 0.5\n",
    "    model_path = \"./output/model_0002999.pth\"\n",
    "    \n",
    "    \n",
    "    cfg = get_cfg()\n",
    "    cfg.merge_from_file(model_zoo.get_config_file(\"COCO-InstanceSegmentation/mask_rcnn_R_50_FPN_3x.yaml\")) \n",
    "    cfg.MODEL.WEIGHTS = model_path\n",
    "    cfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = threshold   # set the testing threshold for this model\n",
    "    cfg.MODEL.ROI_HEADS.NUM_CLASSES = 1\n",
    "    cfg.INPUT.MIN_SIZE_TEST= 300\n",
    "    cfg.INPUT.MAX_SIZE_TEST = 300\n",
    "    cfg.INPUT.FORMAT = 'BGR'\n",
    "    cfg.TEST.DETECTIONS_PER_IMAGE = 1000\n",
    "    \n",
    "    \n",
    "    \n",
    "    predictor = DefaultPredictor(cfg)\n",
    "    results = {}\n",
    "    \n",
    "    \n",
    "    for img in os.listdir(test_images_path):\n",
    "        img_path = os.path.join(test_images_path,img)\n",
    "        im = cv2.imread(img_path)\n",
    "        outputs = predictor(im)\n",
    "        prediction = outputs['instances'].pred_masks.cpu().numpy()\n",
    "        mask = np.add.reduce(prediction)\n",
    "        mask = mask > 0\n",
    "        \n",
    "        results[img] = mask  \n",
    "    np.savez_compressed(predictions_output_path,**results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В функцию передаем два аргумента:\n",
    "* путь к папке с изображениями, на которых хотим сделать предсказания. сейчас это путь к валдиационными изображениям. Папку test мы положим во время запуска контейнера.\n",
    "* путь к создаваемому файлу с предсказаниями. по время локального дебага можно использовать любое имя\n",
    "\n",
    "Этот же код вынесен в отдельные скрипты для удобства, они должны запускаться во время запуска контейнера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run('data/val/images','prediction.npz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Оценка качества."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оценка качества на нашей стороне просходит по следующему скрипту - `evaluate.py`. Он принимает на вход путь к двум файлам формата `npz`. \n",
    "* `ref_path` - путь к файлу с правильными ответами \n",
    "* `pred_path` - путь к файлу с предсказаниями"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для начала сгенерируем файл формата `npz` для валидации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_true = {}\n",
    "for i in loaded_train.files:\n",
    "    if i in val_predictions.keys():\n",
    "        val_true[i] = loaded_train[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez_compressed('val_true.npz',**val_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate.py --ref_path val_true.npz --pred_path val_pred.npz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
